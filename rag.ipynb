{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b584c9d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d0b4a",
   "metadata": {},
   "source": [
    "Импортируем себе данные из БД (на примере SQLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc23eab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def get_data_from_db(db_name=\"your_database.db\"):\n",
    "    \"\"\"\n",
    "    Получение данных из базы данных с добавлением информации о тематике (category)\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT id, text, category FROM documents\")  # Включаем столбец 'category'\n",
    "    data = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ef364",
   "metadata": {},
   "source": [
    "Подготовка данных к векторизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6177ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Пример списка ругательных слов\n",
    "bad_words = {\"глупый\", \"тупой\", \"идиот\", \"дурак\", \"пошел вон\", \"сука\"}  # Можно расширить\n",
    "\n",
    "# Функция для препроцессинга текста\n",
    "def preprocess_text(text):\n",
    "    # Приводим текст к нижнему регистру\n",
    "    text = text.lower()\n",
    "\n",
    "    # Удаляем ругательные слова\n",
    "    text = \" \".join([word for word in text.split() if word not in bad_words])\n",
    "\n",
    "    # Удаляем пунктуацию\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Удаляем лишние пробелы\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94308e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metadata.append({\n",
    "    \"document_id\": doc_id,\n",
    "    \"chunk_idx\": chunk_idx,\n",
    "    \"text\": chunk,\n",
    "    \"full_text_length\": len(text),\n",
    "    \"chunk_text_length\": len(chunk),\n",
    "    \"category\": category  # Добавляем категорию\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ca715",
   "metadata": {},
   "source": [
    "Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4662b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def vectorize_with_gigachat(text):\n",
    "    response = requests.post(\"http://localhost:5000/v1/encode\", json={\"text\": text})\n",
    "    vector = response.json().get('vector')  # Пример получения вектора из ответа\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672ff7f",
   "metadata": {},
   "source": [
    "индексация с помощью FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f73897",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(vectors):\n",
    "    dimension = len(vectors[0])  # Должно соответствовать размерности вектора\n",
    "    index = faiss.IndexFlatL2(dimension)  # Индекс по L2-метрике\n",
    "\n",
    "    # Преобразование векторов в массив numpy\n",
    "    vectors_np = np.array(vectors).astype(np.float32)\n",
    "    \n",
    "    # Добавление векторов в индекс\n",
    "    index.add(vectors_np)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656873a",
   "metadata": {},
   "source": [
    "Нужно развернуть qdrant в докере, чтобы можно было потестить локально\n",
    "docker pull qdrant/qdrant\n",
    "docker run -p 6333:6333 qdrant/qdrant\n",
    "\n",
    "Запись в qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f5c2d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "def write_to_qdrant(vectors, metadata):\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)  # Порт по умолчанию\n",
    "    collection_name = \"documents_collection\"\n",
    "\n",
    "    # Преобразование векторов в точки\n",
    "    points = [PointStruct(id=i, vector=vec, payload={\"metadata\": meta}) for i, (vec, meta) in enumerate(zip(vectors, metadata))]\n",
    "\n",
    "    # Создание коллекции и запись точек\n",
    "    client.upsert(collection_name=collection_name, points=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e106e4",
   "metadata": {},
   "source": [
    "реализация поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5d92c",
   "metadata": {},
   "source": [
    "def search_in_qdrant(query_vector):\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    collection_name = \"documents_collection\"\n",
    "\n",
    "    # Запрос к Qdrant для поиска по вектору\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        top=5  # Топ-5 наиболее похожих результатов\n",
    "    )\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6517d",
   "metadata": {},
   "source": [
    "Генерация овета с OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fc989",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_answer(prompt):\n",
    "    openai.api_key = \"your-openai-api-key\"\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46a029",
   "metadata": {},
   "source": [
    "собираем вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4f709",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def rag_system_with_preprocessing(query):\n",
    "    try:\n",
    "        # 1. Получение данных из базы данных\n",
    "        db_data = get_data_from_db()\n",
    "\n",
    "        vectors = []\n",
    "        metadata = []\n",
    "        all_chunks = []  # Сюда будем собирать все чанки с метаданными\n",
    "\n",
    "        for doc_id, text, category in db_data:  # Теперь получаем 'category'\n",
    "            try:\n",
    "                # 2. Применяем препроцессинг к тексту\n",
    "                processed_text = preprocess_text(text)\n",
    "\n",
    "                # 3. Разбиваем текст на чанки\n",
    "                chunks = split_text_into_chunks(processed_text)\n",
    "\n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    try:\n",
    "                        # 4. Векторизация каждого чанка\n",
    "                        vector = vectorize_with_gigachat(chunk)\n",
    "                        vectors.append(vector)\n",
    "\n",
    "                        # 5. Добавление метаданных\n",
    "                        metadata.append({\n",
    "                            \"document_id\": doc_id,\n",
    "                            \"chunk_idx\": chunk_idx,\n",
    "                            \"text\": chunk,\n",
    "                            \"full_text_length\": len(text),\n",
    "                            \"chunk_text_length\": len(chunk),\n",
    "                            \"category\": category  # Добавляем категорию в метаданные\n",
    "                        })\n",
    "                        all_chunks.append(chunk)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Ошибка в векторизации чанка {chunk_idx} документа {doc_id}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка в обработке документа {doc_id}: {e}\")\n",
    "\n",
    "        # 6. Индексация в FAISS\n",
    "        faiss_index = build_faiss_index(vectors)\n",
    "\n",
    "        # 7. Запись в Qdrant с добавлением метаданных\n",
    "        write_to_qdrant(vectors, metadata)\n",
    "\n",
    "        # 8. Применяем препроцессинг к запросу пользователя\n",
    "        processed_query = preprocess_text(query)\n",
    "\n",
    "        # 9. Векторизация запроса пользователя\n",
    "        query_vector = vectorize_with_gigachat(processed_query)\n",
    "\n",
    "        # 10. Поиск в Qdrant, ограничиваем результат до 3\n",
    "        search_results = search_in_qdrant(query_vector, top=3)\n",
    "\n",
    "        # 11. Генерация ответа через OpenAI\n",
    "        top_result = search_results[0]  # Лучший результат\n",
    "        response_text = top_result.payload['metadata']['text']\n",
    "\n",
    "        prompt = f\"На основе следующего контекста: {response_text}\\nОтветьте на следующий запрос: {query}\"\n",
    "        answer = generate_answer(prompt)\n",
    "\n",
    "        # Вывод ответа\n",
    "        print(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка в основной системе: {e}\")\n",
    "\n",
    "# Пример использования\n",
    "query = \"Какие новые технологии использованы в нашей системе?\"\n",
    "rag_system_with_preprocessing(query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
